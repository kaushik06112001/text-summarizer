{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2gs80st8Alm",
        "outputId": "377be74e-c12e-409b-cd4a-d27f719e3eed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def text_summarizer(text, num_sentences=3):\n",
        "    # Tokenize text into sentences\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    # Tokenize words and remove stopwords\n",
        "    words = word_tokenize(text)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = [word for word in words if word.lower() not in stop_words and word.isalnum()]\n",
        "\n",
        "    # Calculate word frequency\n",
        "    freq_dist = FreqDist(words)\n",
        "\n",
        "    # Score sentences based on word frequency\n",
        "    sentence_scores = {}\n",
        "    for sentence in sentences: #to iterate through all the sentences\n",
        "        for word in word_tokenize(sentence.lower()): #to iterate through all the words in a sentence\n",
        "            if word in freq_dist: #to exclude the stopwords and only iterate through words that are in the freq distribution table\n",
        "                if sentence not in sentence_scores:             #if the sentence is not in the sentence_score dictionary put the freq of the word from the freq_dist table as value\n",
        "                    sentence_scores[sentence] = freq_dist[word] #against the sentence\n",
        "                else:                                           #else add the freq of the word t0 the sentence_score\n",
        "                    sentence_scores[sentence] += freq_dist[word]\n",
        "\n",
        "    # sort the sentences in descending order and select the required no. of sentences from the top\n",
        "    top_sentences = sorted(sentence_scores, key=sentence_scores.get, reverse=True)[:num_sentences] #sort the sentences in descending order and select the required no. of sentences\n",
        "\n",
        "    # Generate summary\n",
        "    summary = ' '.join(top_sentences)\n",
        "    return summary\n",
        ""
      ],
      "metadata": {
        "id": "dDo2a2lW8chZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "to select the sentences that have the words with highest frequencies"
      ],
      "metadata": {
        "id": "AP8Bl5SL7Z4V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Example usage\n",
        "text=input(\"\\nEnter the text u want to summarize:\\n\")\n",
        "n=int(input(\"\\nEnter the no. of sentences: \"))\n",
        "summary = text_summarizer(text)\n",
        "print(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IrJRhm4i8hf3",
        "outputId": "565c82bb-ac99-422c-e782-49e02bbc236f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Enter the text u want to summarize:\n",
            "Over recent years, machine learning methods, with their ability to make sense of vast amounts of data and automate decisions, have found widespread applications in healthcare, robotics, biology, physics, consumer products, internet services, and various other industries. Giant leaps in science usually come from a combination of powerful ideas and great tools. Machine learning is no exception. The success of data-driven learning methods is based on the ingenious ideas of thousands of talented researchers over the field’s 60-year history. But their recent popularity is also fueled by the evolution of hardware and software solutions that make them scalable and accessible. The ecosystem of excellent libraries for numeric computing, data analysis, and machine learning built around Python like NumPy and scikit-learn gained wide adoption in research and industry. This has greatly helped propel Python to be the most popular programming language. Massive improvements in computer vision, text, speech, and other tasks brought by the recent advent of deep learning techniques exemplify this theme. Approaches draw on neural network theory of the last four decades that started working remarkably well in combination with GPUs and highly optimized compute routines. Our goal with building PyTorch over the past five years has been to give researchers the most flexible tool for expressing deep learning algorithms while taking care of the underlying engineering complexities. We benefited from the excellent Python ecosystem. In turn, we’ve been fortunate to see the community of very talented people build advanced deep learning models across various domains on top of PyTorch. The authors of this book were among them. I’ve known Sebastian within this tight-knit community for a few years now. He has unmatched talent in easily explaining information and making the complex accessible. Sebastian contributed to many widely used machine learning software packages and authored dozens of excellent tutorials on deep learning and data visualization. Mastery of both ideas and tools is also required to apply machine learning in practice. Getting started might feel intimidating, from making sense of theoretical concepts to figuring out which software packages to install\n",
            "\n",
            "Enter the no. of sentences: 3\n",
            "Text-->>sentence\n",
            " ['Over recent years, machine learning methods, with their ability to make sense of vast amounts of data and automate decisions, have found widespread applications in healthcare, robotics, biology, physics, consumer products, internet services, and various other industries.', 'Giant leaps in science usually come from a combination of powerful ideas and great tools.', 'Machine learning is no exception.', 'The success of data-driven learning methods is based on the ingenious ideas of thousands of talented researchers over the field’s 60-year history.', 'But their recent popularity is also fueled by the evolution of hardware and software solutions that make them scalable and accessible.', 'The ecosystem of excellent libraries for numeric computing, data analysis, and machine learning built around Python like NumPy and scikit-learn gained wide adoption in research and industry.', 'This has greatly helped propel Python to be the most popular programming language.', 'Massive improvements in computer vision, text, speech, and other tasks brought by the recent advent of deep learning techniques exemplify this theme.', 'Approaches draw on neural network theory of the last four decades that started working remarkably well in combination with GPUs and highly optimized compute routines.', 'Our goal with building PyTorch over the past five years has been to give researchers the most flexible tool for expressing deep learning algorithms while taking care of the underlying engineering complexities.', 'We benefited from the excellent Python ecosystem.', 'In turn, we’ve been fortunate to see the community of very talented people build advanced deep learning models across various domains on top of PyTorch.', 'The authors of this book were among them.', 'I’ve known Sebastian within this tight-knit community for a few years now.', 'He has unmatched talent in easily explaining information and making the complex accessible.', 'Sebastian contributed to many widely used machine learning software packages and authored dozens of excellent tutorials on deep learning and data visualization.', 'Mastery of both ideas and tools is also required to apply machine learning in practice.', 'Getting started might feel intimidating, from making sense of theoretical concepts to figuring out which software packages to install'] \n",
            "\n",
            "Words after removing stopwords are:\n",
            " ['recent', 'years', 'machine', 'learning', 'methods', 'ability', 'make', 'sense', 'vast', 'amounts', 'data', 'automate', 'decisions', 'found', 'widespread', 'applications', 'healthcare', 'robotics', 'biology', 'physics', 'consumer', 'products', 'internet', 'services', 'various', 'industries', 'Giant', 'leaps', 'science', 'usually', 'come', 'combination', 'powerful', 'ideas', 'great', 'tools', 'Machine', 'learning', 'exception', 'success', 'learning', 'methods', 'based', 'ingenious', 'ideas', 'thousands', 'talented', 'researchers', 'field', 'history', 'recent', 'popularity', 'also', 'fueled', 'evolution', 'hardware', 'software', 'solutions', 'make', 'scalable', 'accessible', 'ecosystem', 'excellent', 'libraries', 'numeric', 'computing', 'data', 'analysis', 'machine', 'learning', 'built', 'around', 'Python', 'like', 'NumPy', 'gained', 'wide', 'adoption', 'research', 'industry', 'greatly', 'helped', 'propel', 'Python', 'popular', 'programming', 'language', 'Massive', 'improvements', 'computer', 'vision', 'text', 'speech', 'tasks', 'brought', 'recent', 'advent', 'deep', 'learning', 'techniques', 'exemplify', 'theme', 'Approaches', 'draw', 'neural', 'network', 'theory', 'last', 'four', 'decades', 'started', 'working', 'remarkably', 'well', 'combination', 'GPUs', 'highly', 'optimized', 'compute', 'routines', 'goal', 'building', 'PyTorch', 'past', 'five', 'years', 'give', 'researchers', 'flexible', 'tool', 'expressing', 'deep', 'learning', 'algorithms', 'taking', 'care', 'underlying', 'engineering', 'complexities', 'benefited', 'excellent', 'Python', 'ecosystem', 'turn', 'fortunate', 'see', 'community', 'talented', 'people', 'build', 'advanced', 'deep', 'learning', 'models', 'across', 'various', 'domains', 'top', 'PyTorch', 'authors', 'book', 'among', 'known', 'Sebastian', 'within', 'community', 'years', 'unmatched', 'talent', 'easily', 'explaining', 'information', 'making', 'complex', 'accessible', 'Sebastian', 'contributed', 'many', 'widely', 'used', 'machine', 'learning', 'software', 'packages', 'authored', 'dozens', 'excellent', 'tutorials', 'deep', 'learning', 'data', 'visualization', 'Mastery', 'ideas', 'tools', 'also', 'required', 'apply', 'machine', 'learning', 'practice', 'Getting', 'started', 'might', 'feel', 'intimidating', 'making', 'sense', 'theoretical', 'concepts', 'figuring', 'software', 'packages', 'install']\n",
            "Freq of each word\n",
            " <FreqDist with 168 samples and 214 outcomes>\n",
            "Over recent years, machine learning methods, with their ability to make sense of vast amounts of data and automate decisions, have found widespread applications in healthcare, robotics, biology, physics, consumer products, internet services, and various other industries. Sebastian contributed to many widely used machine learning software packages and authored dozens of excellent tutorials on deep learning and data visualization. The ecosystem of excellent libraries for numeric computing, data analysis, and machine learning built around Python like NumPy and scikit-learn gained wide adoption in research and industry.\n"
          ]
        }
      ]
    }
  ]
}